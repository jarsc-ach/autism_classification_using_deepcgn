{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.4981 - loss: 17.4154 - val_accuracy: 0.5417 - val_loss: 7.7925\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5382 - loss: 15.4147 - val_accuracy: 0.5833 - val_loss: 1.2102\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5836 - loss: 8.3755 - val_accuracy: 0.4583 - val_loss: 6.0754\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5112 - loss: 16.3854 - val_accuracy: 1.0000 - val_loss: 0.0728\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4997 - loss: 10.8929 - val_accuracy: 0.5417 - val_loss: 5.7020\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4881 - loss: 9.6966 - val_accuracy: 0.5417 - val_loss: 1.9266\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4972 - loss: 8.4207 - val_accuracy: 0.9167 - val_loss: 0.2248\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6324 - loss: 5.4712 - val_accuracy: 0.4583 - val_loss: 2.1566\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5750 - loss: 6.1606 - val_accuracy: 0.4583 - val_loss: 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an audio file...\n",
      "No file selected.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load dataset (Ensure you have preprocessed audio files and labels)\n",
    "def load_audio_data(data_dir):\n",
    "    X, y = [], []\n",
    "    for label in [\"Autism\", \"Non autism\"]:\n",
    "        class_dir = os.path.join(data_dir, label)\n",
    "        for file in os.listdir(class_dir):\n",
    "            file_path = os.path.join(class_dir, file)\n",
    "            y_audio, sr = librosa.load(file_path, sr=22050, mono=True)\n",
    "            mfcc = librosa.feature.mfcc(y=y_audio, sr=sr, n_mfcc=40)\n",
    "            mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "            X.append(mfcc_scaled)\n",
    "            y.append(0 if label == \"Autism\" else 1)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load Data\n",
    "data_dir = \"D://autism early sathyabhama//autism//audio datasets//\"  # Update with your dataset path\n",
    "X, y = load_audio_data(data_dir)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(40,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# Save Model\n",
    "model.save(\"audio_classification_model.h5\")\n",
    "\n",
    "# Load pre-trained model\n",
    "model_path = \"audio_classification_model.h5\"\n",
    "audio_model = load_model(model_path)\n",
    "\n",
    "# Define class labels\n",
    "audio_labels = {0: 'Autism', 1: 'Non autism'}\n",
    "\n",
    "# Function to process and predict audio\n",
    "def process_and_predict_audio(audio_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=22050, mono=True)  # Load audio file\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)  # Extract MFCC features\n",
    "        mfcc_scaled = np.mean(mfcc.T, axis=0)  # Scale features\n",
    "        mfcc_scaled = np.expand_dims(mfcc_scaled, axis=0)  # Add batch dimension\n",
    "\n",
    "        prediction = audio_model.predict(mfcc_scaled)\n",
    "        predicted_class = int(np.argmax(prediction, axis=-1)[0])\n",
    "        result = audio_labels[predicted_class]\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Select audio file\n",
    "print(\"Select an audio file...\")\n",
    "audio_path = \"03-01-01-01-01-01-01.wav\"  # Replace with actual file path for testing\n",
    "if os.path.exists(audio_path):\n",
    "    print(f\"Processing: {audio_path}\")\n",
    "    prediction_result = process_and_predict_audio(audio_path)\n",
    "    print(f\"Predicted Classification: {prediction_result}\")\n",
    "\n",
    "    # Display waveform\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(\"Audio Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
